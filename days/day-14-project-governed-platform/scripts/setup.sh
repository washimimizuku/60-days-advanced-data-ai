#!/bin/bash

# DataCorp Governed Platform Setup Script

echo "ğŸš€ Setting up DataCorp Governed Data Platform..."

# Check prerequisites
echo "ğŸ“‹ Checking prerequisites..."
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker is not installed. Please install Docker first."
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "âŒ Docker Compose is not installed. Please install Docker Compose first."
    exit 1
fi

# Copy environment template if .env doesn't exist
if [ ! -f .env ]; then
    echo "ğŸ“ Creating environment configuration..."
    cp .env.example .env
    
    # Generate Fernet key for Airflow
    echo "ğŸ” Generating Airflow Fernet key..."
    python3 -c "from cryptography.fernet import Fernet; print('AIRFLOW_FERNET_KEY=' + Fernet.generate_key().decode())" >> .env
fi

# Create required directories
echo "ğŸ“ Creating directory structure..."
mkdir -p {airflow/{dags,logs,plugins},dbt/{models,tests,macros},governance/{policies,scripts,logs},monitoring/{prometheus,grafana,alerts},sql,sample_data}

# Set proper permissions
echo "ğŸ”’ Setting permissions..."
chmod -R 755 airflow/
chmod -R 755 governance/scripts/
chmod +x scripts/setup.sh

# Start core services first
echo "ğŸ—„ï¸ Starting core services..."
docker-compose up -d postgres airflow-db redis

# Wait for databases to be ready
echo "â³ Waiting for databases to initialize..."
sleep 30

# Check database health
echo "ğŸ¥ Checking database health..."
docker-compose exec postgres pg_isready -U platform_user || {
    echo "âŒ PostgreSQL is not ready. Check logs: docker-compose logs postgres"
    exit 1
}

# Initialize Airflow database
echo "âœˆï¸ Initializing Airflow..."
docker-compose run --rm airflow-webserver airflow db init

# Create Airflow admin user
echo "ğŸ‘¤ Creating Airflow admin user..."
docker-compose run --rm airflow-webserver airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@datacorp.com \
    --password admin

# Start all services
echo "ğŸƒ Starting all services..."
docker-compose up -d

# Wait for services to be ready
echo "â³ Waiting for services to be ready..."
sleep 60

# Health check
echo "ğŸ¥ Running health checks..."
services=("airflow-webserver:8080/health" "grafana:3000/api/health" "prometheus:9090/-/healthy")
for service in "${services[@]}"; do
    if curl -f "http://localhost:${service#*:}" &> /dev/null; then
        echo "âœ… ${service%:*} is healthy"
    else
        echo "âš ï¸ ${service%:*} may not be ready yet"
    fi
done

# Load sample data
echo "ğŸ“Š Loading sample data..."
docker-compose exec postgres psql -U platform_user -d datacorp_platform -f /docker-entrypoint-initdb.d/init.sql

# Setup dbt
echo "ğŸ”§ Setting up dbt..."
docker-compose exec airflow-webserver dbt deps --project-dir /opt/dbt || echo "âš ï¸ dbt deps failed - this is normal on first run"

# Enable DAGs
echo "ğŸ“‹ Enabling Airflow DAGs..."
sleep 10
docker-compose exec airflow-webserver airflow dags unpause governance_daily_pipeline || echo "âš ï¸ DAG not found yet - will be available after restart"

echo ""
echo "âœ… DataCorp Governed Platform setup complete!"
echo ""
echo "ğŸŒ Access URLs:"
echo "   Airflow: http://localhost:8080 (admin/admin)"
echo "   Grafana: http://localhost:3000 (admin/admin)"
echo "   DataHub: http://localhost:8090"
echo "   Prometheus: http://localhost:9090"
echo ""
echo "ğŸ“š Next steps:"
echo "   1. Visit Airflow to see governance DAGs"
echo "   2. Check Grafana for governance dashboards"
echo "   3. Review governance policies in ./governance/policies/"
echo "   4. Run: docker-compose exec airflow-webserver airflow dags trigger governance_daily_pipeline"
echo ""
echo "ğŸ†˜ Troubleshooting:"
echo "   - Check logs: docker-compose logs <service-name>"
echo "   - Health check: docker-compose ps"
echo "   - Restart: docker-compose restart"