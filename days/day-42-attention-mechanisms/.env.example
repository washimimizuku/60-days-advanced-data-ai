# Day 42: Attention Mechanisms - Environment Configuration

# PyTorch Configuration
TORCH_DEVICE=cpu  # or cuda if GPU available
TORCH_DTYPE=float32  # or float16 for mixed precision

# Performance Settings
BATCH_SIZE=4
MAX_SEQ_LEN=2048
NUM_WORKERS=4

# Attention Configuration
DEFAULT_D_MODEL=512
DEFAULT_NUM_HEADS=8
DEFAULT_DROPOUT=0.1

# Sparse Attention Settings
SPARSE_PATTERN=local  # local, strided, random, none
WINDOW_SIZE=256

# Visualization Settings
SAVE_PLOTS=true
PLOT_DIR=./plots
MAX_TOKENS_DISPLAY=20

# Memory Management
USE_GRADIENT_CHECKPOINTING=false
USE_MIXED_PRECISION=false
CACHE_SIZE=1024

# Development Settings
DEBUG_MODE=false
VERBOSE_LOGGING=true
SEED=42