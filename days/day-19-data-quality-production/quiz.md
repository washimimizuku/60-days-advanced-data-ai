# Day 19 Quiz: Data Quality in Production - Frameworks, Monitoring, Automation

## Questions

### 1. Which data quality dimension focuses on ensuring all required fields contain values?
- a) Accuracy
- b) Completeness
- c) Consistency
- d) Timeliness

### 2. In Great Expectations, what is the primary purpose of an expectation suite?
- a) To store validation results
- b) To define a collection of data quality rules for a dataset
- c) To configure database connections
- d) To generate data documentation

### 3. What should be the completeness threshold for financial transaction data in production?
- a) 95%
- b) 99%
- c) 100%
- d) 90%

### 4. Which component in Great Expectations is responsible for automated validation execution and alerting?
- a) Expectation Suite
- b) Data Context
- c) Checkpoint
- d) Batch Request

### 5. In a data contract, what does SLA typically specify?
- a) Data schema definitions
- b) Service level agreements including availability and performance targets
- c) Data transformation logic
- d) User access permissions

### 6. What is the recommended approach for handling data quality failures in critical financial pipelines?
- a) Log the error and continue processing
- b) Send a notification but continue processing
- c) Stop the pipeline immediately and trigger critical alerts
- d) Retry the validation three times before failing

### 7. Which alerting severity should be used when customer data completeness drops below the required threshold?
- a) Low
- b) Medium
- c) High
- d) Critical

### 8. What is the primary benefit of integrating data quality checks into Airflow DAGs?
- a) Faster query execution
- b) Better data visualization
- c) Automated quality gates that prevent bad data from flowing downstream
- d) Reduced storage costs

### 9. In production data quality monitoring, what does "data freshness" measure?
- a) How recently the data was created or updated
- b) The accuracy of the data values
- c) The completeness of required fields
- d) The consistency across systems

### 10. Which approach provides the most comprehensive data quality monitoring?
- a) Manual data validation once per week
- b) Automated validation with real-time monitoring and multi-dimensional quality scoring
- c) Simple row count checks
- d) Schema validation only

---

## Answers

### 1. Which data quality dimension focuses on ensuring all required fields contain values?
**Answer: b) Completeness**

**Explanation:** Completeness is the data quality dimension that measures whether all required data fields contain values (are not null or empty). This includes checking for missing values, null fields, and ensuring that critical business data is present. In production systems, completeness is often the first quality check performed because downstream processes typically cannot function properly with missing critical data.

---

### 2. In Great Expectations, what is the primary purpose of an expectation suite?
**Answer: b) To define a collection of data quality rules for a dataset**

**Explanation:** An expectation suite in Great Expectations is a collection of expectations (data quality rules) that define what "good data" looks like for a specific dataset. It contains multiple expectation configurations that test various aspects of data quality such as completeness, uniqueness, format validation, and business rules. The suite serves as a comprehensive quality specification that can be executed against data batches to validate quality.

---

### 3. What should be the completeness threshold for financial transaction data in production?
**Answer: c) 100%**

**Explanation:** Financial transaction data requires 100% completeness for critical fields due to regulatory requirements, audit trails, and business integrity. Missing transaction IDs, amounts, timestamps, or customer IDs can lead to compliance violations, financial discrepancies, and audit failures. Unlike other data types where 95-99% completeness might be acceptable, financial data demands perfect completeness for core transactional fields.

---

### 4. Which component in Great Expectations is responsible for automated validation execution and alerting?
**Answer: c) Checkpoint**

**Explanation:** Checkpoints in Great Expectations are responsible for running validation suites automatically and handling the results through configured actions. They orchestrate the validation process, execute expectation suites against data batches, and trigger actions like storing results, updating documentation, sending notifications, or stopping pipelines based on validation outcomes. Checkpoints are the automation layer that makes Great Expectations production-ready.

---

### 5. In a data contract, what does SLA typically specify?
**Answer: b) Service level agreements including availability and performance targets**

**Explanation:** SLAs (Service Level Agreements) in data contracts specify measurable service commitments including system availability (e.g., 99.9% uptime), performance targets (e.g., query response times), data quality thresholds (e.g., minimum quality scores), and error rate limits. SLAs provide clear expectations for data consumers and establish accountability for data providers, enabling data-as-a-product approaches.

---

### 6. What is the recommended approach for handling data quality failures in critical financial pipelines?
**Answer: c) Stop the pipeline immediately and trigger critical alerts**

**Explanation:** Critical financial pipelines should implement a "fail-fast" approach where quality failures immediately stop processing and trigger high-severity alerts (PagerDuty, immediate notifications). This prevents propagation of bad financial data that could lead to regulatory violations, incorrect financial reporting, or business decisions based on flawed data. The cost of stopping the pipeline is much lower than the cost of processing incorrect financial data.

---

### 7. Which alerting severity should be used when customer data completeness drops below the required threshold?
**Answer: c) High**

**Explanation:** Customer data completeness issues warrant high-severity alerts because they directly impact business operations like marketing campaigns, customer analytics, and ML model performance. While not as critical as financial data failures, customer data quality issues can significantly affect business decisions and customer experience, requiring prompt attention from the data team but not necessarily immediate on-call response.

---

### 8. What is the primary benefit of integrating data quality checks into Airflow DAGs?
**Answer: c) Automated quality gates that prevent bad data from flowing downstream**

**Explanation:** Integrating quality checks into Airflow DAGs creates automated quality gates that validate data before it proceeds to downstream tasks. This prevents bad data from propagating through the pipeline, contaminating analytics, ML models, or reports. Quality checks act as circuit breakers that stop processing when data doesn't meet standards, ensuring data reliability throughout the entire pipeline.

---

### 9. In production data quality monitoring, what does "data freshness" measure?
**Answer: a) How recently the data was created or updated**

**Explanation:** Data freshness (also called timeliness) measures how recently data was created, updated, or made available for consumption. It's typically measured as the time elapsed since the data's timestamp or last update. Fresh data is crucial for real-time analytics, operational dashboards, and time-sensitive business decisions. Stale data can lead to incorrect insights and poor business outcomes.

---

### 10. Which approach provides the most comprehensive data quality monitoring?
**Answer: b) Automated validation with real-time monitoring and multi-dimensional quality scoring**

**Explanation:** Comprehensive data quality monitoring requires automated validation across multiple quality dimensions (completeness, accuracy, consistency, timeliness, uniqueness), real-time monitoring with trend analysis, multi-dimensional scoring that provides overall quality metrics, and proactive alerting. This approach provides continuous visibility into data health, enables quick issue detection, and supports data-driven quality improvement initiatives.

---

## Score Interpretation

- **9-10 correct**: Data Quality Expert! You understand production-grade quality frameworks and monitoring
- **7-8 correct**: Strong quality knowledge! Review alerting strategies and SLA management
- **5-6 correct**: Good foundation! Focus on Great Expectations and production patterns
- **3-4 correct**: Basic understanding present! Study quality dimensions and monitoring approaches
- **Below 3**: Review the theory section and practice with quality frameworks

---

## Key Concepts to Remember

### Data Quality Dimensions
1. **Completeness** measures presence of required data values and absence of nulls
2. **Accuracy** validates correctness of data values against business rules and formats
3. **Consistency** ensures data alignment across systems and time periods
4. **Timeliness** measures data freshness and update frequency
5. **Uniqueness** validates absence of duplicates where business rules require uniqueness

### Great Expectations Framework
6. **Expectation suites** define comprehensive quality rules for datasets
7. **Checkpoints** automate validation execution and result handling
8. **Data context** manages configuration, stores, and execution environment
9. **Batch requests** specify which data to validate
10. **Actions** handle validation results through notifications, documentation, and pipeline control

### Production Quality Patterns
11. **Fail-fast approach** stops processing immediately when critical quality checks fail
12. **Quality gates** prevent bad data from flowing to downstream systems
13. **Multi-dimensional scoring** provides comprehensive quality assessment
14. **Trend analysis** identifies quality degradation patterns over time
15. **Automated alerting** enables proactive response to quality issues

### Data Contracts and SLAs
16. **Service level agreements** specify measurable quality and performance commitments
17. **Quality thresholds** define acceptable levels for each quality dimension
18. **Stakeholder alignment** ensures clear expectations between data producers and consumers
19. **Compliance requirements** drive quality standards for regulated data
20. **Monitoring and reporting** provide visibility into contract adherence

### Alerting and Incident Response
21. **Severity-based routing** sends alerts through appropriate channels based on impact
22. **Escalation procedures** ensure critical issues receive timely attention
23. **Runbook integration** provides clear response procedures for quality incidents
24. **Multi-channel notifications** use Slack, email, PagerDuty based on severity
25. **Automated remediation** can fix certain quality issues without human intervention

### Integration Patterns
- **Airflow integration** embeds quality checks as pipeline tasks with dependencies
- **dbt integration** uses tests and macros for transformation-time validation
- **CI/CD integration** validates data quality in deployment pipelines
- **Real-time monitoring** provides continuous visibility into data health
- **Dashboard integration** displays quality metrics and trends for stakeholders

### Common Anti-Patterns to Avoid
- **Manual quality checks** that don't scale and are error-prone
- **Ignoring quality failures** and allowing bad data to propagate
- **Inconsistent thresholds** across similar datasets without business justification
- **Alert fatigue** from too many low-priority notifications
- **Lack of documentation** making quality rules unclear to stakeholders
- **No trend analysis** missing gradual quality degradation patterns

Ready to move on to Day 20! ðŸš€