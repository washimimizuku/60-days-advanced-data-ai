# Day 48: Fine-tuning Techniques - LoRA & QLoRA Requirements

# Core ML and Deep Learning
torch>=2.0.0
transformers>=4.35.0
accelerate>=0.24.0
datasets>=2.14.0

# Parameter-Efficient Fine-Tuning
peft>=0.6.0
bitsandbytes>=0.41.0

# Quantization and Optimization
optimum>=1.14.0
auto-gptq>=0.4.0

# Data Processing and Utilities
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0
tqdm>=4.65.0

# Evaluation and Metrics
evaluate>=0.4.0
rouge-score>=0.1.2
sacrebleu>=2.3.0

# Monitoring and Logging
wandb>=0.15.0
tensorboard>=2.14.0
matplotlib>=3.7.0
seaborn>=0.12.0

# Memory Profiling and Optimization
psutil>=5.9.0
py3nvml>=0.2.7
memory-profiler>=0.61.0

# Configuration Management
omegaconf>=2.3.0
hydra-core>=1.3.0

# Development and Testing
pytest>=7.4.0
pytest-cov>=4.1.0
black>=23.7.0
isort>=5.12.0
flake8>=6.0.0

# Jupyter and Visualization
jupyter>=1.0.0
ipywidgets>=8.1.0
plotly>=5.15.0

# Optional: For advanced quantization techniques
# ggml-python>=0.1.0
# llama-cpp-python>=0.2.0