# Day 24: Project - Production Pipeline with Quality & Monitoring

## ðŸ“– Learning Objectives (30 min)

By the end of today, you will:
- **Integrate** all Phase 2 concepts into a comprehensive production data platform
- **Build** an enterprise-grade pipeline combining Airflow orchestration, dbt transformations, and real-time streaming
- **Implement** comprehensive data quality validation, observability monitoring, and testing strategies
- **Deploy** a complete production system with AWS Glue serverless ETL and Kinesis streaming integration
- **Demonstrate** advanced data engineering skills through a complex, multi-technology integration project

â­ **Difficulty**: Advanced Integration Project (2 hours)

---

## Theory

### What is a Production Data Pipeline?

A production data pipeline is a comprehensive, enterprise-grade system that reliably processes data from source to destination while maintaining quality, observability, and governance standards. It integrates multiple technologies and patterns to create a robust, scalable, and maintainable data platform.

**Why Production Pipelines Matter**:
- **Business Continuity**: Reliable data processing for critical business operations
- **Data Quality**: Automated validation and monitoring ensure trustworthy data
- **Operational Excellence**: Comprehensive monitoring, alerting, and incident response
- **Scalability**: Handle growing data volumes and complexity
- **Compliance**: Meet regulatory and governance requirements
- **Cost Efficiency**: Optimize resource usage and operational overhead

### Phase 2 Integration Architecture

Our production pipeline integrates all technologies learned in Phase 2:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Production Data Platform Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Data Sources   â”‚    â”‚   Processing    â”‚    â”‚    Outputs      â”‚         â”‚
â”‚  â”‚                 â”‚    â”‚     Layer       â”‚    â”‚                 â”‚         â”‚
â”‚  â”‚ â€¢ Databases     â”‚â”€â”€â”€â–¶â”‚                 â”‚â”€â”€â”€â–¶â”‚ â€¢ Data Warehouseâ”‚         â”‚
â”‚  â”‚ â€¢ APIs          â”‚    â”‚ â€¢ Airflow       â”‚    â”‚ â€¢ Analytics     â”‚         â”‚
â”‚  â”‚ â€¢ Files         â”‚    â”‚ â€¢ dbt           â”‚    â”‚ â€¢ Dashboards    â”‚         â”‚
â”‚  â”‚ â€¢ Streams       â”‚    â”‚ â€¢ Glue ETL      â”‚    â”‚ â€¢ ML Features   â”‚         â”‚
â”‚  â”‚ â€¢ Real-time     â”‚    â”‚ â€¢ Kinesis       â”‚    â”‚ â€¢ APIs          â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Quality & Governance Layer                       â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  â€¢ Data Quality (Day 19): Great Expectations, validation rules     â”‚   â”‚
â”‚  â”‚  â€¢ Observability (Day 20): Metrics, logging, distributed tracing  â”‚   â”‚
â”‚  â”‚  â€¢ Testing (Day 21): Unit, integration, end-to-end test suites    â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Infrastructure & Monitoring                      â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  â€¢ Prometheus + Grafana: Metrics collection and visualization      â”‚   â”‚
â”‚  â”‚  â€¢ ELK Stack: Centralized logging and search                       â”‚   â”‚
â”‚  â”‚  â€¢ Jaeger: Distributed tracing for complex workflows               â”‚   â”‚
â”‚  â”‚  â€¢ PagerDuty: Intelligent alerting and incident management         â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Integration Patterns

#### 1. Orchestration-First Architecture (Days 15-16)
```python
# Advanced Airflow DAG with comprehensive patterns
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.task_group import TaskGroup
from airflow_dbt.operators.dbt_operator import DbtRunOperator, DbtTestOperator

dag = DAG(
    'production_data_pipeline',
    schedule_interval='@hourly',
    catchup=False,
    max_active_runs=1,
    tags=['production', 'phase2-integration']
)

# Dynamic task groups for scalability
with TaskGroup('data_ingestion', dag=dag) as ingestion_group:
    # Multiple source ingestion tasks
    for source in ['customers', 'transactions', 'products']:
        ingest_task = PythonOperator(
            task_id=f'ingest_{source}',
            python_callable=ingest_data_source,
            op_kwargs={'source': source}
        )

# Quality gates before transformation
with TaskGroup('quality_validation', dag=dag) as quality_group:
    data_quality_check = PythonOperator(
        task_id='validate_data_quality',
        python_callable=run_great_expectations_suite
    )
    
    schema_validation = PythonOperator(
        task_id='validate_schema',
        python_callable=validate_data_schemas
    )

# dbt transformations with comprehensive testing
with TaskGroup('transformations', dag=dag) as transform_group:
    dbt_run = DbtRunOperator(
        task_id='dbt_run',
        dir='/opt/dbt',
        profiles_dir='/opt/dbt/profiles'
    )
    
    dbt_test = DbtTestOperator(
        task_id='dbt_test',
        dir='/opt/dbt',
        profiles_dir='/opt/dbt/profiles'
    )

# Real-time streaming integration
streaming_integration = PythonOperator(
    task_id='kinesis_stream_processing',
    python_callable=process_kinesis_streams,
    dag=dag
)

# Dependencies with comprehensive error handling
ingestion_group >> quality_group >> transform_group >> streaming_integration
```
#### 2. Advanced dbt Transformations (Days 17-18)
```sql
-- Advanced dbt model with comprehensive features
{{ config(
    materialized='incremental',
    unique_key='customer_id',
    on_schema_change='fail',
    tags=['customer_analytics', 'production'],
    meta={
        'owner': 'data_engineering_team',
        'description': 'Customer 360 view with advanced analytics',
        'sla_hours': 2
    }
) }}

with customer_base as (
    select
        customer_id,
        first_name,
        last_name,
        email,
        registration_date,
        customer_segment,
        -- Advanced window functions for analytics
        row_number() over (
            partition by email 
            order by registration_date desc
        ) as email_rank,
        -- Custom macro for data quality
        {{ validate_email('email') }} as email_is_valid,
        -- Incremental processing logic
        updated_at
    from {{ source('raw', 'customers') }}
    {% if is_incremental() %}
        where updated_at > (select max(updated_at) from {{ this }})
    {% endif %}
),

transaction_metrics as (
    select
        customer_id,
        count(*) as total_transactions,
        sum(amount) as total_spent,
        avg(amount) as avg_transaction_amount,
        max(transaction_date) as last_transaction_date,
        -- Advanced analytics with dbt-utils
        {{ dbt_utils.datediff('min(transaction_date)', 'max(transaction_date)', 'day') }} as customer_lifetime_days
    from {{ ref('stg_transactions') }}
    group by customer_id
),

final as (
    select
        cb.customer_id,
        cb.first_name,
        cb.last_name,
        cb.email,
        cb.registration_date,
        cb.customer_segment,
        cb.email_is_valid,
        tm.total_transactions,
        tm.total_spent,
        tm.avg_transaction_amount,
        tm.last_transaction_date,
        tm.customer_lifetime_days,
        -- Business logic for customer scoring
        case
            when tm.total_spent > 10000 then 'high_value'
            when tm.total_spent > 1000 then 'medium_value'
            else 'low_value'
        end as customer_value_segment,
        current_timestamp as updated_at
    from customer_base cb
    left join transaction_metrics tm on cb.customer_id = tm.customer_id
    where cb.email_rank = 1  -- Deduplication
      and cb.email_is_valid = true  -- Quality filter
)

select * from final
```

#### 3. Production Data Quality (Day 19)
```python
# Great Expectations integration with Airflow
import great_expectations as ge
from great_expectations.checkpoint import SimpleCheckpoint

class DataQualityValidator:
    """Production data quality validation with Great Expectations"""
    
    def __init__(self, data_context_path: str):
        self.context = ge.get_context(context_root_dir=data_context_path)
    
    def validate_batch_data(self, datasource_name: str, data_asset_name: str) -> dict:
        """Validate batch data with comprehensive expectations"""
        
        # Get batch data
        batch_request = {
            'datasource_name': datasource_name,
            'data_connector_name': 'default_inferred_data_connector_name',
            'data_asset_name': data_asset_name
        }
        
        # Create validator
        validator = self.context.get_validator(
            batch_request=batch_request,
            expectation_suite_name=f'{data_asset_name}_suite'
        )
        
        # Run comprehensive expectations
        expectations = [
            # Data completeness
            validator.expect_table_row_count_to_be_between(min_value=1000, max_value=10000000),
            validator.expect_column_values_to_not_be_null('customer_id'),
            validator.expect_column_values_to_be_unique('customer_id'),
            
            # Data validity
            validator.expect_column_values_to_match_regex('email', r'^[^@]+@[^@]+\.[^@]+$'),
            validator.expect_column_values_to_be_between('transaction_amount', min_value=0, max_value=100000),
            
            # Business rules
            validator.expect_column_values_to_be_in_set('customer_segment', ['premium', 'standard', 'basic']),
            validator.expect_column_pair_values_A_to_be_greater_than_B('updated_at', 'created_at'),
            
            # Statistical expectations
            validator.expect_column_mean_to_be_between('transaction_amount', min_value=50, max_value=500),
            validator.expect_column_stdev_to_be_between('transaction_amount', min_value=10, max_value=1000)
        ]
        
        # Create checkpoint and run validation
        checkpoint = SimpleCheckpoint(
            name=f'{data_asset_name}_checkpoint',
            data_context=self.context,
            validator=validator
        )
        
        results = checkpoint.run()
        
        return {
            'success': results['success'],
            'statistics': results['run_results'],
            'failed_expectations': [
                exp for exp in results['run_results'] 
                if not exp['success']
            ]
        }
```

#### 4. Comprehensive Observability (Day 20)
```python
# Production observability with multiple monitoring systems
import logging
import time
from prometheus_client import Counter, Histogram, Gauge
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Metrics collection
PIPELINE_RUNS = Counter('pipeline_runs_total', 'Total pipeline runs', ['status', 'pipeline'])
PROCESSING_TIME = Histogram('processing_time_seconds', 'Time spent processing', ['stage'])
DATA_QUALITY_SCORE = Gauge('data_quality_score', 'Current data quality score', ['dataset'])

# Distributed tracing setup
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

jaeger_exporter = JaegerExporter(
    agent_host_name="jaeger",
    agent_port=6831,
)

span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

class ProductionObservability:
    """Comprehensive observability for production pipelines"""
    
    def __init__(self):
        self.logger = self._setup_structured_logging()
    
    def _setup_structured_logging(self):
        """Setup structured logging with correlation IDs"""
        logger = logging.getLogger('production_pipeline')
        logger.setLevel(logging.INFO)
        
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '{"timestamp": "%(asctime)s", "level": "%(levelname)s", '
            '"component": "%(name)s", "message": "%(message)s", '
            '"correlation_id": "%(correlation_id)s"}'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    @tracer.start_as_current_span("pipeline_execution")
    def monitor_pipeline_execution(self, pipeline_name: str, execution_func):
        """Monitor pipeline execution with comprehensive observability"""
        
        correlation_id = f"pipeline_{int(time.time())}"
        start_time = time.time()
        
        try:
            self.logger.info(
                f"Starting pipeline execution: {pipeline_name}",
                extra={'correlation_id': correlation_id}
            )
            
            # Execute pipeline with monitoring
            with PROCESSING_TIME.labels(stage='total').time():
                result = execution_func()
            
            # Record success metrics
            PIPELINE_RUNS.labels(status='success', pipeline=pipeline_name).inc()
            
            execution_time = time.time() - start_time
            self.logger.info(
                f"Pipeline completed successfully: {pipeline_name} in {execution_time:.2f}s",
                extra={'correlation_id': correlation_id}
            )
            
            return result
            
        except Exception as e:
            # Record failure metrics
            PIPELINE_RUNS.labels(status='failure', pipeline=pipeline_name).inc()
            
            self.logger.error(
                f"Pipeline failed: {pipeline_name} - {str(e)}",
                extra={'correlation_id': correlation_id}
            )
            
            # Send alert
            self._send_alert(pipeline_name, str(e), correlation_id)
            raise
    
    def _send_alert(self, pipeline_name: str, error_message: str, correlation_id: str):
        """Send production alerts for pipeline failures"""
        # Integration with PagerDuty, Slack, etc.
        pass
```

#### 5. Testing Strategies Integration (Day 21)
```python
# Comprehensive testing framework for production pipelines
import pytest
import pandas as pd
from unittest.mock import Mock, patch
from airflow.models import DagBag
from great_expectations.data_context import DataContext

class TestProductionPipeline:
    """Comprehensive test suite for production data pipeline"""
    
    @pytest.fixture
    def sample_data(self):
        """Generate sample data for testing"""
        return pd.DataFrame({
            'customer_id': range(1, 1001),
            'email': [f'user{i}@example.com' for i in range(1, 1001)],
            'transaction_amount': [100.0 + i for i in range(1000)],
            'customer_segment': ['premium'] * 300 + ['standard'] * 400 + ['basic'] * 300
        })
    
    def test_dag_structure(self):
        """Test DAG structure and dependencies"""
        dagbag = DagBag(dag_folder='dags/', include_examples=False)
        
        # Test DAG loads without errors
        assert len(dagbag.import_errors) == 0, f"DAG import errors: {dagbag.import_errors}"
        
        # Test specific DAG exists
        dag = dagbag.get_dag('production_data_pipeline')
        assert dag is not None
        
        # Test task dependencies
        expected_tasks = ['data_ingestion', 'quality_validation', 'transformations']
        actual_tasks = [task.task_id for task in dag.tasks]
        
        for task in expected_tasks:
            assert task in actual_tasks, f"Missing task: {task}"
    
    def test_data_quality_validation(self, sample_data):
        """Test data quality validation logic"""
        validator = DataQualityValidator('/opt/great_expectations')
        
        # Mock Great Expectations context
        with patch.object(validator, 'context') as mock_context:
            mock_validator = Mock()
            mock_context.get_validator.return_value = mock_validator
            
            # Test validation passes with good data
            mock_validator.expect_table_row_count_to_be_between.return_value = {'success': True}
            mock_validator.expect_column_values_to_not_be_null.return_value = {'success': True}
            
            result = validator.validate_batch_data('test_source', 'customers')
            assert result['success'] is True
    
    def test_dbt_model_logic(self, sample_data):
        """Test dbt model transformations"""
        # Test customer segmentation logic
        def calculate_customer_value_segment(total_spent):
            if total_spent > 10000:
                return 'high_value'
            elif total_spent > 1000:
                return 'medium_value'
            else:
                return 'low_value'
        
        # Test segmentation logic
        assert calculate_customer_value_segment(15000) == 'high_value'
        assert calculate_customer_value_segment(5000) == 'medium_value'
        assert calculate_customer_value_segment(500) == 'low_value'
    
    def test_integration_end_to_end(self):
        """Test complete pipeline integration"""
        # This would test the entire pipeline flow
        # from data ingestion to final output
        pass
    
    def test_error_handling(self):
        """Test pipeline error handling and recovery"""
        # Test various failure scenarios
        pass
    
    def test_performance_benchmarks(self, sample_data):
        """Test pipeline performance meets SLAs"""
        start_time = time.time()
        
        # Simulate data processing
        processed_data = sample_data.copy()
        processed_data['processed_at'] = pd.Timestamp.now()
        
        processing_time = time.time() - start_time
        
        # Assert processing time meets SLA (e.g., < 30 seconds for 1000 records)
        assert processing_time < 30, f"Processing took {processing_time:.2f}s, exceeds SLA"
```

#### 6. AWS Integration (Days 22-23)
```python
# AWS Glue and Kinesis integration for serverless and streaming
import boto3
from awsglue.context import GlueContext
from awsglue.transforms import *
from pyspark.context import SparkContext

class AWSIntegrationManager:
    """Manage AWS Glue and Kinesis integration"""
    
    def __init__(self):
        self.glue_client = boto3.client('glue')
        self.kinesis_client = boto3.client('kinesis')
        self.firehose_client = boto3.client('firehose')
    
    def create_glue_etl_job(self, job_name: str, script_location: str):
        """Create production Glue ETL job"""
        
        job_config = {
            'Name': job_name,
            'Role': 'arn:aws:iam::account:role/GlueServiceRole',
            'Command': {
                'Name': 'glueetl',
                'ScriptLocation': script_location,
                'PythonVersion': '3'
            },
            'DefaultArguments': {
                '--job-language': 'python',
                '--job-bookmark-option': 'job-bookmark-enable',
                '--enable-metrics': 'true',
                '--enable-continuous-cloudwatch-log': 'true'
            },
            'MaxRetries': 2,
            'Timeout': 2880,  # 48 hours
            'GlueVersion': '3.0',
            'NumberOfWorkers': 10,
            'WorkerType': 'G.1X'
        }
        
        return self.glue_client.create_job(**job_config)
    
    def setup_kinesis_streaming(self, stream_name: str):
        """Setup Kinesis streaming for real-time processing"""
        
        # Create Kinesis Data Stream
        stream_config = {
            'StreamName': stream_name,
            'ShardCount': 5,
            'StreamModeDetails': {
                'StreamMode': 'PROVISIONED'
            }
        }
        
        self.kinesis_client.create_stream(**stream_config)
        
        # Create Firehose delivery stream
        firehose_config = {
            'DeliveryStreamName': f'{stream_name}-firehose',
            'DeliveryStreamType': 'KinesisStreamAsSource',
            'KinesisStreamSourceConfiguration': {
                'KinesisStreamARN': f'arn:aws:kinesis:region:account:stream/{stream_name}',
                'RoleARN': 'arn:aws:iam::account:role/firehose_delivery_role'
            },
            'ExtendedS3DestinationConfiguration': {
                'RoleARN': 'arn:aws:iam::account:role/firehose_delivery_role',
                'BucketARN': 'arn:aws:s3:::production-data-lake',
                'Prefix': 'streaming-data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/',
                'BufferingHints': {
                    'SizeInMBs': 128,
                    'IntervalInSeconds': 60
                },
                'CompressionFormat': 'GZIP',
                'DataFormatConversionConfiguration': {
                    'Enabled': True,
                    'OutputFormatConfiguration': {
                        'Serializer': {
                            'ParquetSerDe': {}
                        }
                    }
                }
            }
        }
        
        return self.firehose_client.create_delivery_stream(**firehose_config)
```

### Production Deployment Patterns

#### Infrastructure as Code
```yaml
# docker-compose.yml for complete production environment
version: '3.8'
services:
  # Airflow Services
  airflow-webserver:
    image: apache/airflow:2.7.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://redis:6379/0
      - AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt:/opt/dbt
      - ./great_expectations:/opt/great_expectations
    ports:
      - "8080:8080"
    depends_on:
      - postgres
      - redis
  
  airflow-scheduler:
    image: apache/airflow:2.7.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt:/opt/dbt
    depends_on:
      - postgres
      - redis
  
  # Data Infrastructure
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
  
  redis:
    image: redis:7
    ports:
      - "6379:6379"
  
  # Monitoring Stack
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
  
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
  
  # Logging
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.15.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
  
  kibana:
    image: docker.elastic.co/kibana/kibana:7.15.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
  
  # Distributed Tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"
      - "14268:14268"
    environment:
      - COLLECTOR_ZIPKIN_HTTP_PORT=9411

volumes:
  postgres_data:
  grafana_data:
```

---

## ðŸ’» Project Implementation (2 hours)

### Project Overview

**Company**: "DataFlow Enterprises"  
**Role**: Senior Data Platform Engineer  
**Challenge**: Build a comprehensive production data platform integrating all Phase 2 technologies  
**Timeline**: 2 hours for MVP, extensible for production deployment  

**Business Context**: DataFlow Enterprises processes customer data from multiple sources and needs a robust, scalable platform that ensures data quality, provides real-time insights, and maintains comprehensive observability for business-critical operations.

### Architecture Requirements

1. **Multi-Source Data Ingestion**: Handle batch and streaming data
2. **Orchestrated Processing**: Airflow-managed workflows with dbt transformations
3. **Quality Assurance**: Automated validation with Great Expectations
4. **Real-time Processing**: Kinesis streaming integration
5. **Serverless ETL**: AWS Glue for scalable processing
6. **Comprehensive Monitoring**: Observability across all components
7. **Testing Coverage**: Unit, integration, and end-to-end tests

See `project.md` for detailed implementation steps and deliverables.

---

## ðŸ“š Resources

- **Apache Airflow**: [airflow.apache.org](https://airflow.apache.org/) - Production orchestration patterns
- **dbt Documentation**: [docs.getdbt.com](https://docs.getdbt.com/) - Advanced transformation techniques
- **Great Expectations**: [greatexpectations.io](https://greatexpectations.io/) - Data quality validation
- **AWS Glue**: [docs.aws.amazon.com/glue](https://docs.aws.amazon.com/glue/) - Serverless ETL
- **Kinesis**: [docs.aws.amazon.com/kinesis](https://docs.aws.amazon.com/kinesis/) - Real-time streaming
- **Prometheus**: [prometheus.io](https://prometheus.io/) - Metrics collection
- **Grafana**: [grafana.com](https://grafana.com/) - Observability dashboards

---

## ðŸŽ¯ Key Takeaways

- **Integration complexity** requires careful architecture planning and modular design
- **Production systems** need comprehensive monitoring, alerting, and incident response capabilities
- **Data quality** must be automated and integrated throughout the pipeline, not added as an afterthought
- **Observability** provides the foundation for reliable operations and continuous improvement
- **Testing strategies** ensure reliability and enable confident deployments to production
- **Cloud services** like AWS Glue and Kinesis provide scalable, managed infrastructure for enterprise workloads
- **Infrastructure as Code** enables reproducible, version-controlled deployments
- **Multi-technology integration** requires understanding of each component's strengths and integration patterns

---

## ðŸš€ What's Next?

Tomorrow (Day 25), you'll begin **Phase 3: Advanced ML & MLOps** with **Feature Stores - Feast & Tecton**.

**Preview**: You'll learn how to build and manage feature stores for machine learning, including feature engineering, serving, and monitoring. This represents a major shift from data engineering to ML engineering, building on the solid data platform foundation you've established in Phases 1 and 2!

---

## âœ… Before Moving On

- [ ] Understand how to integrate multiple data technologies into cohesive platforms
- [ ] Can build production-grade pipelines with comprehensive quality and monitoring
- [ ] Know how to implement testing strategies across complex, multi-component systems
- [ ] Understand observability patterns for distributed data systems
- [ ] Can deploy and manage production data infrastructure
- [ ] Complete the comprehensive integration project
- [ ] Review all Phase 2 concepts and their integration patterns

**Time spent**: ~2 hours  
**Difficulty**: â­â­â­â­â­ (Advanced Integration Project)

**Congratulations on completing Phase 2: Data Orchestration & Quality!** ðŸŽ‰  
You now have enterprise-grade skills in production data platform engineering! ðŸš€