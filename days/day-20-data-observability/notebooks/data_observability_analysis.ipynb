{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 20: Data Observability - Interactive Analysis\n",
    "\n",
    "This notebook provides interactive analysis of the data observability system, allowing you to explore monitoring results, anomaly detection, and alerting patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import psycopg2\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ“Š Data Observability Analysis Environment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Database Connection and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection\n",
    "db_config = {\n",
    "    'host': 'postgres',\n",
    "    'port': '5432',\n",
    "    'database': 'observability_db',\n",
    "    'user': 'obs_user',\n",
    "    'password': 'obs_password'\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_config)\n",
    "    print(\"âœ… Connected to PostgreSQL database\")\n",
    "    \n",
    "    # Load sample data\n",
    "    transactions_df = pd.read_sql(\"\"\"\n",
    "        SELECT * FROM data_sources.customer_transactions \n",
    "        ORDER BY created_at DESC LIMIT 1000\n",
    "    \"\"\", conn)\n",
    "    \n",
    "    metrics_df = pd.read_sql(\"\"\"\n",
    "        SELECT * FROM monitoring.data_quality_metrics \n",
    "        ORDER BY created_at DESC\n",
    "    \"\"\", conn)\n",
    "    \n",
    "    alerts_df = pd.read_sql(\"\"\"\n",
    "        SELECT * FROM alerts.alert_history \n",
    "        ORDER BY created_at DESC\n",
    "    \"\"\", conn)\n",
    "    \n",
    "    print(f\"ðŸ“Š Loaded {len(transactions_df)} transactions, {len(metrics_df)} metrics, {len(alerts_df)} alerts\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Database connection failed: {e}\")\n",
    "    print(\"ðŸ’¡ Make sure Docker containers are running: docker-compose up -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"ðŸ“ˆ Transaction Data Overview:\")\n",
    "print(f\"   â€¢ Total Transactions: {len(transactions_df):,}\")\n",
    "print(f\"   â€¢ Date Range: {transactions_df['created_at'].min()} to {transactions_df['created_at'].max()}\")\n",
    "print(f\"   â€¢ Unique Customers: {transactions_df['customer_id'].nunique():,}\")\n",
    "print(f\"   â€¢ Total Amount: ${transactions_df['amount'].sum():,.2f}\")\n",
    "\n",
    "# Show data quality summary\n",
    "print(\"\\nðŸ” Data Quality Summary:\")\n",
    "print(f\"   â€¢ Missing Values: {transactions_df.isnull().sum().sum()}\")\n",
    "print(f\"   â€¢ Duplicate Transactions: {transactions_df.duplicated().sum()}\")\n",
    "print(f\"   â€¢ Status Distribution: {transactions_df['status'].value_counts().to_dict()}\")\n",
    "\n",
    "# Display sample data\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Five Pillars Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freshness Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Data Observability - Five Pillars Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Freshness - Time since last update\n",
    "transactions_df['hours_since_created'] = (datetime.now() - pd.to_datetime(transactions_df['created_at'])).dt.total_seconds() / 3600\n",
    "axes[0, 0].hist(transactions_df['hours_since_created'], bins=30, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Data Freshness (Hours Since Creation)')\n",
    "axes[0, 0].set_xlabel('Hours')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# 2. Volume - Daily transaction counts\n",
    "daily_counts = transactions_df.groupby(transactions_df['created_at'].dt.date).size()\n",
    "axes[0, 1].plot(daily_counts.index, daily_counts.values, marker='o', color='green')\n",
    "axes[0, 1].set_title('Data Volume (Daily Transaction Counts)')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Transaction Count')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Distribution - Amount distribution\n",
    "axes[1, 0].hist(transactions_df['amount'], bins=30, alpha=0.7, color='orange')\n",
    "axes[1, 0].set_title('Data Distribution (Transaction Amounts)')\n",
    "axes[1, 0].set_xlabel('Amount ($)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Schema - Status categories\n",
    "status_counts = transactions_df['status'].value_counts()\n",
    "axes[1, 1].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%')\n",
    "axes[1, 1].set_title('Data Schema (Status Distribution)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Five Pillars Analysis Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Anomaly Detection Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate anomaly detection on transaction amounts\n",
    "amounts = transactions_df['amount'].values\n",
    "\n",
    "# Calculate Z-scores\n",
    "mean_amount = np.mean(amounts)\n",
    "std_amount = np.std(amounts)\n",
    "z_scores = (amounts - mean_amount) / std_amount\n",
    "\n",
    "# Identify anomalies (|z-score| > 2)\n",
    "anomaly_threshold = 2.0\n",
    "anomalies = np.abs(z_scores) > anomaly_threshold\n",
    "anomaly_indices = np.where(anomalies)[0]\n",
    "\n",
    "# Create interactive plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Normal points\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(len(amounts))[~anomalies],\n",
    "    y=amounts[~anomalies],\n",
    "    mode='markers',\n",
    "    name='Normal Transactions',\n",
    "    marker=dict(color='blue', size=4, opacity=0.6)\n",
    "))\n",
    "\n",
    "# Anomaly points\n",
    "if len(anomaly_indices) > 0:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=anomaly_indices,\n",
    "        y=amounts[anomaly_indices],\n",
    "        mode='markers',\n",
    "        name='Anomalies',\n",
    "        marker=dict(color='red', size=8, symbol='x')\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Anomaly Detection in Transaction Amounts',\n",
    "    xaxis_title='Transaction Index',\n",
    "    yaxis_title='Amount ($)',\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"ðŸŽ¯ Anomaly Detection Results:\")\n",
    "print(f\"   â€¢ Total Transactions: {len(amounts):,}\")\n",
    "print(f\"   â€¢ Anomalies Detected: {len(anomaly_indices)}\")\n",
    "print(f\"   â€¢ Anomaly Rate: {len(anomaly_indices)/len(amounts)*100:.2f}%\")\n",
    "if len(anomaly_indices) > 0:\n",
    "    print(f\"   â€¢ Anomalous Amounts: {amounts[anomaly_indices]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alert Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze alert patterns if we have alert data\n",
    "if len(alerts_df) > 0:\n",
    "    print(\"ðŸš¨ Alert Analysis:\")\n",
    "    \n",
    "    # Alert severity distribution\n",
    "    severity_counts = alerts_df['severity'].value_counts()\n",
    "    print(f\"   â€¢ Severity Distribution: {severity_counts.to_dict()}\")\n",
    "    \n",
    "    # Alert status distribution\n",
    "    status_counts = alerts_df['status'].value_counts()\n",
    "    print(f\"   â€¢ Status Distribution: {status_counts.to_dict()}\")\n",
    "    \n",
    "    # Create alert timeline\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Severity over time\n",
    "    severity_colors = {'low': 'green', 'medium': 'orange', 'high': 'red', 'critical': 'darkred'}\n",
    "    for severity in alerts_df['severity'].unique():\n",
    "        severity_data = alerts_df[alerts_df['severity'] == severity]\n",
    "        ax1.scatter(pd.to_datetime(severity_data['created_at']), \n",
    "                   severity_data['current_value'], \n",
    "                   c=severity_colors.get(severity, 'blue'), \n",
    "                   label=severity, alpha=0.7, s=50)\n",
    "    \n",
    "    ax1.set_title('Alert Timeline by Severity')\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Alert Value')\n",
    "    ax1.legend()\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Alert frequency by metric\n",
    "    metric_counts = alerts_df['metric_name'].value_counts()\n",
    "    ax2.bar(metric_counts.index, metric_counts.values, color='skyblue')\n",
    "    ax2.set_title('Alert Frequency by Metric')\n",
    "    ax2.set_xlabel('Metric Name')\n",
    "    ax2.set_ylabel('Alert Count')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"â„¹ï¸ No alert data available for analysis\")\n",
    "    \n",
    "    # Create sample alert simulation\n",
    "    print(\"ðŸŽ­ Simulating Alert Patterns...\")\n",
    "    \n",
    "    # Generate sample alert data\n",
    "    sample_alerts = pd.DataFrame({\n",
    "        'metric_name': ['data_freshness', 'data_volume', 'schema_change', 'distribution_anomaly'] * 5,\n",
    "        'severity': np.random.choice(['low', 'medium', 'high'], 20),\n",
    "        'created_at': pd.date_range(start='2024-01-01', periods=20, freq='H'),\n",
    "        'current_value': np.random.uniform(0.5, 1.0, 20)\n",
    "    })\n",
    "    \n",
    "    # Plot simulated alerts\n",
    "    fig = px.scatter(sample_alerts, \n",
    "                     x='created_at', \n",
    "                     y='current_value',\n",
    "                     color='severity',\n",
    "                     size='current_value',\n",
    "                     hover_data=['metric_name'],\n",
    "                     title='Simulated Alert Timeline')\n",
    "    fig.show()\n",
    "    \n",
    "    print(\"ðŸ“Š Simulated alert patterns displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Health Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive data health score\n",
    "def calculate_data_health_score(df):\n",
    "    \"\"\"Calculate overall data health score based on multiple factors\"\"\"\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    # 1. Freshness Score (based on recency)\n",
    "    hours_since_latest = (datetime.now() - pd.to_datetime(df['created_at']).max()).total_seconds() / 3600\n",
    "    freshness_score = max(0, 1 - (hours_since_latest / 24))  # Degrade over 24 hours\n",
    "    scores['freshness'] = freshness_score\n",
    "    \n",
    "    # 2. Volume Score (based on expected volume)\n",
    "    expected_daily_volume = 1000  # Expected transactions per day\n",
    "    actual_daily_volume = len(df) / ((pd.to_datetime(df['created_at']).max() - pd.to_datetime(df['created_at']).min()).days + 1)\n",
    "    volume_score = min(1.0, actual_daily_volume / expected_daily_volume)\n",
    "    scores['volume'] = volume_score\n",
    "    \n",
    "    # 3. Completeness Score (based on missing values)\n",
    "    completeness_score = 1 - (df.isnull().sum().sum() / (len(df) * len(df.columns)))\n",
    "    scores['completeness'] = completeness_score\n",
    "    \n",
    "    # 4. Consistency Score (based on data types and formats)\n",
    "    consistency_score = 1.0  # Simplified - assume consistent\n",
    "    scores['consistency'] = consistency_score\n",
    "    \n",
    "    # 5. Validity Score (based on business rules)\n",
    "    valid_amounts = (df['amount'] > 0) & (df['amount'] < 10000)  # Reasonable amount range\n",
    "    valid_quantities = (df['quantity'] > 0) & (df['quantity'] <= 100)  # Reasonable quantity range\n",
    "    validity_score = (valid_amounts & valid_quantities).mean()\n",
    "    scores['validity'] = validity_score\n",
    "    \n",
    "    # Calculate weighted overall score\n",
    "    weights = {\n",
    "        'freshness': 0.25,\n",
    "        'volume': 0.20,\n",
    "        'completeness': 0.20,\n",
    "        'consistency': 0.15,\n",
    "        'validity': 0.20\n",
    "    }\n",
    "    \n",
    "    overall_score = sum(scores[metric] * weights[metric] for metric in weights)\n",
    "    scores['overall'] = overall_score\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Calculate health scores\n",
    "health_scores = calculate_data_health_score(transactions_df)\n",
    "\n",
    "# Display results\n",
    "print(\"ðŸ¥ Data Health Assessment:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, score in health_scores.items():\n",
    "    status = \"âœ… Excellent\" if score >= 0.9 else \"ðŸŸ¡ Good\" if score >= 0.7 else \"ðŸŸ  Fair\" if score >= 0.5 else \"ðŸ”´ Poor\"\n",
    "    print(f\"   â€¢ {metric.capitalize()}: {score:.3f} {status}\")\n",
    "\n",
    "# Create health score visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "metrics = list(health_scores.keys())[:-1]  # Exclude overall\n",
    "values = [health_scores[metric] for metric in metrics]\n",
    "\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=values,\n",
    "    theta=metrics,\n",
    "    fill='toself',\n",
    "    name='Data Health Scores'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 1]\n",
    "        )\n",
    "    ),\n",
    "    title=\"Data Health Score Radar Chart\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Overall Data Health Score: {health_scores['overall']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recommendations and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on analysis\n",
    "def generate_recommendations(health_scores, anomaly_count, alert_count):\n",
    "    \"\"\"Generate actionable recommendations based on analysis results\"\"\"\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Health score based recommendations\n",
    "    if health_scores['freshness'] < 0.8:\n",
    "        recommendations.append({\n",
    "            'priority': 'High',\n",
    "            'category': 'Freshness',\n",
    "            'issue': 'Data freshness below optimal threshold',\n",
    "            'action': 'Increase data ingestion frequency or optimize ETL pipelines'\n",
    "        })\n",
    "    \n",
    "    if health_scores['volume'] < 0.7:\n",
    "        recommendations.append({\n",
    "            'priority': 'Medium',\n",
    "            'category': 'Volume',\n",
    "            'issue': 'Data volume lower than expected',\n",
    "            'action': 'Investigate data source issues or validate filtering logic'\n",
    "        })\n",
    "    \n",
    "    if health_scores['completeness'] < 0.9:\n",
    "        recommendations.append({\n",
    "            'priority': 'High',\n",
    "            'category': 'Completeness',\n",
    "            'issue': 'Missing data detected',\n",
    "            'action': 'Implement data validation rules and improve data collection processes'\n",
    "        })\n",
    "    \n",
    "    if health_scores['validity'] < 0.8:\n",
    "        recommendations.append({\n",
    "            'priority': 'Medium',\n",
    "            'category': 'Validity',\n",
    "            'issue': 'Invalid data values detected',\n",
    "            'action': 'Strengthen data validation rules and implement business logic checks'\n",
    "        })\n",
    "    \n",
    "    # Anomaly based recommendations\n",
    "    if anomaly_count > len(transactions_df) * 0.05:  # More than 5% anomalies\n",
    "        recommendations.append({\n",
    "            'priority': 'High',\n",
    "            'category': 'Anomalies',\n",
    "            'issue': f'High anomaly rate detected ({anomaly_count} anomalies)',\n",
    "            'action': 'Review anomaly detection thresholds and investigate root causes'\n",
    "        })\n",
    "    \n",
    "    # General recommendations\n",
    "    if health_scores['overall'] >= 0.9:\n",
    "        recommendations.append({\n",
    "            'priority': 'Low',\n",
    "            'category': 'Optimization',\n",
    "            'issue': 'Data quality is excellent',\n",
    "            'action': 'Continue monitoring and consider advanced analytics opportunities'\n",
    "        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = generate_recommendations(health_scores, len(anomaly_indices), len(alerts_df))\n",
    "\n",
    "# Display recommendations\n",
    "print(\"ðŸ’¡ Data Observability Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if recommendations:\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        priority_emoji = {'High': 'ðŸ”´', 'Medium': 'ðŸŸ¡', 'Low': 'ðŸŸ¢'}\n",
    "        emoji = priority_emoji.get(rec['priority'], 'ðŸ”µ')\n",
    "        \n",
    "        print(f\"{i}. {emoji} {rec['category']} ({rec['priority']} Priority)\")\n",
    "        print(f\"   Issue: {rec['issue']}\")\n",
    "        print(f\"   Action: {rec['action']}\")\n",
    "        print()\nelse:\n",
    "    print(\"âœ… No specific recommendations - data quality is optimal!\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(\"   1. Set up automated monitoring alerts\")\n",
    "print(\"   2. Implement data quality rules in production\")\n",
    "print(\"   3. Create executive dashboards for stakeholders\")\n",
    "print(\"   4. Establish data SLA monitoring\")\n",
    "print(\"   5. Build automated remediation workflows\")\n",
    "\n",
    "print(\"\\nðŸ”— Resources:\")\n",
    "print(\"   â€¢ Grafana Dashboards: http://localhost:3000\")\n",
    "print(\"   â€¢ Prometheus Metrics: http://localhost:9090\")\n",
    "print(\"   â€¢ Database Access: psql -h postgres -U obs_user observability_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up database connection\n",
    "if 'conn' in locals():\n",
    "    conn.close()\n",
    "    print(\"ðŸ”Œ Database connection closed\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Data Observability Analysis Complete!\")\n",
    "print(\"Thank you for exploring the observability system.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}