{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 22: AWS Glue & Data Catalog - Interactive Exploration\n",
    "\n",
    "This notebook provides interactive exploration of AWS Glue and Data Catalog capabilities using LocalStack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize AWS clients for LocalStack\n",
    "endpoint_url = 'http://localstack:4566'\n",
    "\n",
    "s3_client = boto3.client('s3', endpoint_url=endpoint_url)\n",
    "glue_client = boto3.client('glue', endpoint_url=endpoint_url)\n",
    "athena_client = boto3.client('athena', endpoint_url=endpoint_url)\n",
    "\n",
    "print(\"‚úÖ AWS clients initialized for LocalStack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore Data Catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Glue databases\n",
    "try:\n",
    "    databases = glue_client.get_databases()\n",
    "    print(f\"üìä Found {len(databases['DatabaseList'])} databases:\")\n",
    "    for db in databases['DatabaseList']:\n",
    "        print(f\"   ‚Ä¢ {db['Name']}: {db.get('Description', 'No description')}\")\nexcept Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  No databases found or error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables in database\n",
    "database_name = 'serverlessdata_analytics'\n",
    "\n",
    "try:\n",
    "    tables = glue_client.get_tables(DatabaseName=database_name)\n",
    "    print(f\"üìã Found {len(tables['TableList'])} tables in {database_name}:\")\n",
    "    for table in tables['TableList']:\n",
    "        print(f\"   ‚Ä¢ {table['Name']}: {len(table['StorageDescriptor']['Columns'])} columns\")\nexcept Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  No tables found or error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore S3 Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List S3 buckets\n",
    "try:\n",
    "    buckets = s3_client.list_buckets()\n",
    "    print(f\"ü™£ Found {len(buckets['Buckets'])} S3 buckets:\")\n",
    "    for bucket in buckets['Buckets']:\n",
    "        print(f\"   ‚Ä¢ {bucket['Name']}\")\nexcept Exception as e:\n",
    "    print(f\"‚ùå Error listing buckets: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data structure\n",
    "bucket_name = 'serverlessdata-datalake'\n",
    "\n",
    "try:\n",
    "    objects = s3_client.list_objects_v2(Bucket=bucket_name, Prefix='raw/', Delimiter='/')\n",
    "    \n",
    "    if 'CommonPrefixes' in objects:\n",
    "        print(f\"üìÅ Data folders in {bucket_name}/raw/:\")\n",
    "        for prefix in objects['CommonPrefixes']:\n",
    "            folder = prefix['Prefix'].rstrip('/')\n",
    "            print(f\"   ‚Ä¢ {folder}\")\n",
    "    \n",
    "    if 'Contents' in objects:\n",
    "        print(f\"\\nüìÑ Files in root:\")\n",
    "        for obj in objects['Contents'][:5]:  # Show first 5 files\n",
    "            print(f\"   ‚Ä¢ {obj['Key']} ({obj['Size']} bytes)\")\n",
    "            \nexcept Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  No objects found or error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and analyze sample transaction data\n",
    "try:\n",
    "    # List transaction files\n",
    "    objects = s3_client.list_objects_v2(\n",
    "        Bucket=bucket_name, \n",
    "        Prefix='raw/transactions/',\n",
    "        MaxKeys=1\n",
    "    )\n",
    "    \n",
    "    if 'Contents' in objects and len(objects['Contents']) > 0:\n",
    "        # Get first transaction file\n",
    "        first_file = objects['Contents'][0]['Key']\n",
    "        print(f\"üìä Analyzing sample file: {first_file}\")\n",
    "        \n",
    "        # Download and read CSV\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=first_file)\n",
    "        df = pd.read_csv(response['Body'])\n",
    "        \n",
    "        print(f\"\\nüìà Data Summary:\")\n",
    "        print(f\"   ‚Ä¢ Records: {len(df):,}\")\n",
    "        print(f\"   ‚Ä¢ Columns: {len(df.columns)}\")\n",
    "        print(f\"   ‚Ä¢ Date range: {df['transaction_date'].min()} to {df['transaction_date'].max()}\")\n",
    "        print(f\"   ‚Ä¢ Total amount: ${df['transaction_amount'].sum():,.2f}\")\n",
    "        \n",
    "        # Show sample records\n",
    "        print(f\"\\nüìã Sample Records:\")\n",
    "        display(df.head())\n",
    "        \n",
    "        # Basic analytics\n",
    "        print(f\"\\nüìä Quick Analytics:\")\n",
    "        print(f\"   ‚Ä¢ Average transaction: ${df['transaction_amount'].mean():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Unique customers: {df['customer_id'].nunique():,}\")\n",
    "        print(f\"   ‚Ä¢ Payment methods: {df['payment_method'].nunique()}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No transaction files found\")\n",
    "        \nexcept Exception as e:\n",
    "    print(f\"‚ùå Error analyzing data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Glue Crawler Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List existing crawlers\n",
    "try:\n",
    "    crawlers = glue_client.get_crawlers()\n",
    "    print(f\"üï∑Ô∏è Found {len(crawlers['Crawlers'])} crawlers:\")\n",
    "    \n",
    "    for crawler in crawlers['Crawlers']:\n",
    "        print(f\"\\n   ‚Ä¢ {crawler['Name']}\")\n",
    "        print(f\"     State: {crawler['State']}\")\n",
    "        print(f\"     Database: {crawler['DatabaseName']}\")\n",
    "        \n",
    "        if 'LastCrawl' in crawler and crawler['LastCrawl']:\n",
    "            last_crawl = crawler['LastCrawl']\n",
    "            print(f\"     Last crawl: {last_crawl.get('Status', 'Unknown')}\")\n",
    "            \nexcept Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  No crawlers found or error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ETL Job Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Glue jobs\n",
    "try:\n",
    "    jobs = glue_client.get_jobs()\n",
    "    print(f\"‚öôÔ∏è Found {len(jobs['Jobs'])} ETL jobs:\")\n",
    "    \n",
    "    for job in jobs['Jobs']:\n",
    "        print(f\"\\n   ‚Ä¢ {job['Name']}\")\n",
    "        print(f\"     Description: {job.get('Description', 'No description')}\")\n",
    "        print(f\"     Max capacity: {job.get('MaxCapacity', 'Not set')} DPUs\")\n",
    "        print(f\"     Timeout: {job.get('Timeout', 'Not set')} minutes\")\n",
    "        \nexcept Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  No jobs found or error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Customer Analytics Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate customer analytics (what Glue ETL would do)\n",
    "try:\n",
    "    # Get sample transaction data\n",
    "    objects = s3_client.list_objects_v2(\n",
    "        Bucket=bucket_name, \n",
    "        Prefix='raw/transactions/',\n",
    "        MaxKeys=3\n",
    "    )\n",
    "    \n",
    "    if 'Contents' in objects:\n",
    "        # Combine multiple files\n",
    "        all_transactions = []\n",
    "        \n",
    "        for obj in objects['Contents']:\n",
    "            response = s3_client.get_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "            df = pd.read_csv(response['Body'])\n",
    "            all_transactions.append(df)\n",
    "        \n",
    "        # Combine all data\n",
    "        combined_df = pd.concat(all_transactions, ignore_index=True)\n",
    "        \n",
    "        print(f\"üìä Customer Analytics (Simulated ETL Output):\")\n",
    "        print(f\"   Total transactions analyzed: {len(combined_df):,}\")\n",
    "        \n",
    "        # Customer segmentation\n",
    "        customer_summary = combined_df.groupby('customer_id').agg({\n",
    "            'transaction_amount': ['sum', 'mean', 'count']\n",
    "        }).round(2)\n",
    "        \n",
    "        customer_summary.columns = ['total_spent', 'avg_amount', 'transaction_count']\n",
    "        customer_summary = customer_summary.reset_index()\n",
    "        \n",
    "        # Add customer segments\n",
    "        customer_summary['segment'] = pd.cut(\n",
    "            customer_summary['total_spent'],\n",
    "            bins=[0, 500, 2000, float('inf')],\n",
    "            labels=['basic', 'standard', 'premium']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüéØ Customer Segmentation:\")\n",
    "        segment_summary = customer_summary['segment'].value_counts()\n",
    "        for segment, count in segment_summary.items():\n",
    "            print(f\"   ‚Ä¢ {segment}: {count:,} customers\")\n",
    "        \n",
    "        print(f\"\\nüí∞ Revenue by Segment:\")\n",
    "        revenue_by_segment = customer_summary.groupby('segment')['total_spent'].sum()\n",
    "        for segment, revenue in revenue_by_segment.items():\n",
    "            print(f\"   ‚Ä¢ {segment}: ${revenue:,.2f}\")\n",
    "        \n",
    "        # Show top customers\n",
    "        print(f\"\\nüèÜ Top 5 Customers:\")\n",
    "        top_customers = customer_summary.nlargest(5, 'total_spent')\n",
    "        display(top_customers)\n",
    "        \nexcept Exception as e:\n",
    "    print(f\"‚ùå Error in analytics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost Optimization Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data for cost optimization opportunities\n",
    "print(\"üí∞ Cost Optimization Analysis:\")\n",
    "\n",
    "try:\n",
    "    # Analyze file sizes and formats\n",
    "    total_size = 0\n",
    "    file_count = 0\n",
    "    \n",
    "    objects = s3_client.list_objects_v2(Bucket=bucket_name, Prefix='raw/')\n",
    "    \n",
    "    if 'Contents' in objects:\n",
    "        for obj in objects['Contents']:\n",
    "            total_size += obj['Size']\n",
    "            file_count += 1\n",
    "    \n",
    "    print(f\"\\nüìä Current Data Stats:\")\n",
    "    print(f\"   ‚Ä¢ Total files: {file_count:,}\")\n",
    "    print(f\"   ‚Ä¢ Total size: {total_size / (1024*1024):.2f} MB\")\n",
    "    print(f\"   ‚Ä¢ Average file size: {(total_size / file_count) / 1024:.2f} KB\")\n",
    "    \n",
    "    print(f\"\\nüí° Optimization Recommendations:\")\n",
    "    print(f\"   ‚Ä¢ Convert CSV to Parquet: ~70% size reduction\")\n",
    "    print(f\"   ‚Ä¢ Apply compression: Additional ~50% reduction\")\n",
    "    print(f\"   ‚Ä¢ Partition by date: Faster queries, lower costs\")\n",
    "    print(f\"   ‚Ä¢ Columnar format: Better for analytics\")\n",
    "    \n",
    "    # Estimate cost savings\n",
    "    current_storage_cost = (total_size / (1024**3)) * 0.023  # $0.023 per GB/month\n",
    "    optimized_storage_cost = current_storage_cost * 0.15  # 85% reduction\n",
    "    monthly_savings = current_storage_cost - optimized_storage_cost\n",
    "    \n",
    "    print(f\"\\nüí∞ Estimated Monthly Costs:\")\n",
    "    print(f\"   ‚Ä¢ Current (CSV): ${current_storage_cost:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Optimized (Parquet): ${optimized_storage_cost:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Monthly savings: ${monthly_savings:.4f}\")\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"‚ùå Error in cost analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ‚úÖ Data Catalog exploration\n",
    "- ‚úÖ S3 data analysis\n",
    "- ‚úÖ Customer analytics simulation\n",
    "- ‚úÖ Cost optimization analysis\n",
    "\n",
    "**Try these next:**\n",
    "1. Run the complete demo: `python demo.py`\n",
    "2. Explore the exercise: `python exercise.py`\n",
    "3. Check the solution: `python solution.py`\n",
    "4. Take the quiz: Review `quiz.md`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}