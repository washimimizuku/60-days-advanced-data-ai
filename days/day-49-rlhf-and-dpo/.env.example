# Day 49: RLHF and DPO - Human Feedback & Preference Learning - Environment Configuration

# =============================================================================
# Model Configuration
# =============================================================================

# Base model settings
BASE_MODEL_NAME="microsoft/DialoGPT-medium"
MODEL_CACHE_DIR="./models"
MAX_MODEL_SIZE_GB=50

# SFT Configuration
SFT_LEARNING_RATE=1e-5
SFT_BATCH_SIZE=16
SFT_GRADIENT_ACCUMULATION_STEPS=4
SFT_NUM_EPOCHS=3
SFT_MAX_LENGTH=512

# Reward Model Configuration
REWARD_MODEL_LEARNING_RATE=1e-5
REWARD_MODEL_BATCH_SIZE=8
REWARD_MODEL_NUM_EPOCHS=3
REWARD_MODEL_MARGIN=0.5
REWARD_MODEL_HIDDEN_SIZE=768

# DPO Configuration
DPO_LEARNING_RATE=1e-6
DPO_BATCH_SIZE=16
DPO_BETA=0.1
DPO_NUM_EPOCHS=3
DPO_WARMUP_RATIO=0.1

# PPO Configuration (for RLHF)
PPO_LEARNING_RATE=1e-6
PPO_BATCH_SIZE=32
PPO_MINI_BATCH_SIZE=4
PPO_KL_COEFF=0.1
PPO_CLIP_RANGE=0.2
PPO_VALUE_COEFF=0.5
PPO_ENTROPY_COEFF=0.01

# =============================================================================
# Human Preference Data Configuration
# =============================================================================

# Data paths
PREFERENCE_DATA_PATH="./data/preferences.json"
SFT_DATA_PATH="./data/sft_demonstrations.json"
VALIDATION_DATA_PATH="./data/validation.json"

# Annotation settings
MIN_ANNOTATORS_PER_EXAMPLE=3
INTER_ANNOTATOR_AGREEMENT_THRESHOLD=0.7
ANNOTATION_TIME_MIN_SECONDS=30
ANNOTATION_TIME_MAX_SECONDS=600

# Quality control
ENABLE_QUALITY_CONTROL=true
EXPERT_VALIDATION_RATE=0.1
ANNOTATION_VALIDATION_THRESHOLD=0.8

# =============================================================================
# Safety and Alignment Configuration
# =============================================================================

# Safety thresholds
TOXICITY_THRESHOLD=0.7
BIAS_THRESHOLD=0.6
SAFETY_SCORE_THRESHOLD=0.8
HARMFULNESS_THRESHOLD=0.3

# Constitutional AI
ENABLE_CONSTITUTIONAL_AI=true
CONSTITUTION_PATH="./config/constitution.txt"
CRITIQUE_MODEL_NAME="microsoft/DialoGPT-medium"

# Content filtering
ENABLE_CONTENT_FILTERING=true
FILTER_TOXICITY=true
FILTER_BIAS=true
FILTER_HARMFUL_CONTENT=true

# =============================================================================
# Monitoring and Evaluation Configuration
# =============================================================================

# Alignment monitoring
ENABLE_ALIGNMENT_MONITORING=true
MONITORING_FREQUENCY_MINUTES=5
DRIFT_DETECTION_THRESHOLD=0.1
ALERT_SAFETY_THRESHOLD=0.8

# Evaluation metrics
EVAL_BATCH_SIZE=32
EVAL_MAX_LENGTH=512
EVAL_FREQUENCY_STEPS=500

# Safety classifiers
TOXICITY_CLASSIFIER="unitary/toxic-bert-base"
BIAS_CLASSIFIER="custom_bias_detector"
FACTUALITY_CLASSIFIER="custom_fact_checker"

# =============================================================================
# Deployment Configuration
# =============================================================================

# Staged rollout
ENABLE_STAGED_ROLLOUT=true
CANARY_PERCENTAGE=1
LIMITED_PERCENTAGE=10
GRADUAL_PERCENTAGE=50
FULL_PERCENTAGE=100

# Rollback conditions
AUTO_ROLLBACK_ENABLED=true
ROLLBACK_SAFETY_THRESHOLD=0.7
ROLLBACK_ERROR_RATE_THRESHOLD=0.05
ROLLBACK_DELAY_MINUTES=5

# Load balancing
TRAFFIC_SPLIT_STRATEGY="weighted_random"
MODEL_SERVING_TIMEOUT=30
MAX_CONCURRENT_REQUESTS=100

# =============================================================================
# Logging and Monitoring
# =============================================================================

# Weights & Biases
WANDB_PROJECT="rlhf-dpo-alignment"
WANDB_ENTITY=""
WANDB_API_KEY=""
ENABLE_WANDB=false

# TensorBoard
TENSORBOARD_LOG_DIR="./logs/tensorboard"
ENABLE_TENSORBOARD=true

# Logging settings
LOG_LEVEL="INFO"
LOG_FILE="./logs/alignment.log"
LOG_INTERVAL=10

# Evaluation logging
EVAL_LOG_PREDICTIONS=true
EVAL_LOG_SAFETY_SCORES=true
EVAL_LOG_PREFERENCE_ACCURACY=true

# =============================================================================
# Hardware and Performance Configuration
# =============================================================================

# CUDA settings
CUDA_VISIBLE_DEVICES="0"
TORCH_CUDA_ARCH_LIST="7.5,8.0,8.6"

# Memory management
ENABLE_GRADIENT_CHECKPOINTING=true
ENABLE_MIXED_PRECISION=true
MAX_GRAD_NORM=1.0

# Distributed training
ENABLE_DISTRIBUTED_TRAINING=false
WORLD_SIZE=1
RANK=0
MASTER_ADDR="localhost"
MASTER_PORT=12355

# DeepSpeed configuration
ENABLE_DEEPSPEED=false
DEEPSPEED_CONFIG_PATH="./config/deepspeed.json"
DEEPSPEED_STAGE=2

# =============================================================================
# Data Processing Configuration
# =============================================================================

# Tokenization
TOKENIZER_CACHE_DIR="./tokenizers"
MAX_SEQUENCE_LENGTH=512
PADDING_SIDE="right"
TRUNCATION_SIDE="right"

# Data preprocessing
PREPROCESSING_NUM_WORKERS=4
REMOVE_UNUSED_COLUMNS=true
SHUFFLE_DATASET=true
DATASET_SEED=42

# Data validation
VALIDATE_DATA_FORMAT=true
CHECK_PREFERENCE_CONSISTENCY=true
REMOVE_DUPLICATE_EXAMPLES=true

# =============================================================================
# Human Feedback Collection Configuration
# =============================================================================

# Annotation interface
ANNOTATION_INTERFACE_TYPE="web"
ANNOTATION_UI_PORT=8080
ENABLE_ANNOTATION_GUIDELINES=true

# Annotator management
MAX_ANNOTATORS=50
ANNOTATOR_QUALIFICATION_THRESHOLD=0.8
ANNOTATOR_PAYMENT_PER_ANNOTATION=0.50

# Feedback collection
COLLECT_THUMBS_UP_DOWN=true
COLLECT_DETAILED_RATINGS=true
COLLECT_FREE_TEXT_FEEDBACK=true
FEEDBACK_SAMPLING_RATE=0.1

# =============================================================================
# API and Integration Configuration
# =============================================================================

# Model serving API
SERVE_MODEL=false
API_HOST="0.0.0.0"
API_PORT=8000
API_WORKERS=4

# External APIs
OPENAI_API_KEY=""
ANTHROPIC_API_KEY=""
PERSPECTIVE_API_KEY=""

# Rate limiting
RATE_LIMIT_PER_MINUTE=60
RATE_LIMIT_PER_HOUR=1000
ENABLE_RATE_LIMITING=true

# =============================================================================
# Security and Privacy Configuration
# =============================================================================

# Data privacy
ANONYMIZE_USER_DATA=true
DATA_RETENTION_DAYS=90
ENABLE_GDPR_COMPLIANCE=true

# Model security
ENABLE_MODEL_ENCRYPTION=false
ENCRYPTION_KEY=""
SECURE_MODEL_STORAGE=true

# Access control
ENABLE_API_AUTHENTICATION=true
API_KEY_REQUIRED=true
ADMIN_API_KEY=""

# =============================================================================
# Development and Debugging Configuration
# =============================================================================

# Debug settings
DEBUG_MODE=false
VERBOSE_LOGGING=false
PROFILE_MEMORY=false
PROFILE_PERFORMANCE=false

# Testing settings
RUN_UNIT_TESTS=false
RUN_INTEGRATION_TESTS=false
TEST_COVERAGE_THRESHOLD=0.8

# Development mode
DEVELOPMENT_MODE=false
AUTO_RELOAD=false
ENABLE_DEBUG_ENDPOINTS=false

# =============================================================================
# Experimental Features Configuration
# =============================================================================

# Advanced alignment techniques
ENABLE_ITERATED_AMPLIFICATION=false
ENABLE_DEBATE_TRAINING=false
ENABLE_RECURSIVE_REWARD_MODELING=false

# Research features
ENABLE_PREFERENCE_UNCERTAINTY=false
ENABLE_ACTIVE_LEARNING=false
ENABLE_MULTI_OBJECTIVE_OPTIMIZATION=false

# Model interpretability
ENABLE_ATTENTION_VISUALIZATION=false
ENABLE_GRADIENT_ANALYSIS=false
ENABLE_ACTIVATION_PATCHING=false

# =============================================================================
# Custom Configuration
# =============================================================================

# Custom model paths
CUSTOM_SFT_MODEL_PATH=""
CUSTOM_REWARD_MODEL_PATH=""
CUSTOM_POLICY_MODEL_PATH=""

# Custom data paths
CUSTOM_PREFERENCE_DATA_PATH=""
CUSTOM_CONSTITUTION_PATH=""
CUSTOM_SAFETY_RULES_PATH=""

# Override settings
OVERRIDE_SAFETY_CHECKS=false
OVERRIDE_QUALITY_CONTROL=false
OVERRIDE_RATE_LIMITS=false