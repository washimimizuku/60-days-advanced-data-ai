# Day 49: RLHF and DPO - Human Feedback & Preference Learning Requirements

# Core ML and Deep Learning
torch>=2.0.0
transformers>=4.35.0
accelerate>=0.24.0
datasets>=2.14.0

# RLHF and Alignment Libraries
trl>=0.7.0
peft>=0.6.0
bitsandbytes>=0.41.0

# Reinforcement Learning
stable-baselines3>=2.0.0
gymnasium>=0.29.0

# Human Preference and Annotation
human-eval>=1.0.0
anthropic>=0.7.0

# Data Processing and Utilities
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0
tqdm>=4.65.0

# Evaluation and Metrics
evaluate>=0.4.0
rouge-score>=0.1.2
sacrebleu>=2.3.0
bert-score>=0.3.13

# Monitoring and Logging
wandb>=0.15.0
tensorboard>=2.14.0
matplotlib>=3.7.0
seaborn>=0.12.0

# Safety and Content Filtering
detoxify>=0.5.0
perspective-api>=0.1.0

# Statistical Analysis
scipy>=1.11.0
statsmodels>=0.14.0

# Configuration Management
omegaconf>=2.3.0
hydra-core>=1.3.0

# Development and Testing
pytest>=7.4.0
pytest-cov>=4.1.0
black>=23.7.0
isort>=5.12.0
flake8>=6.0.0

# Jupyter and Visualization
jupyter>=1.0.0
ipywidgets>=8.1.0
plotly>=5.15.0

# Optional: Advanced alignment tools
# openai>=1.0.0
# cohere>=4.0.0